{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5436584,"sourceType":"datasetVersion","datasetId":2286927}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BT4014 Project (Group 6)\n## MMA Bandit Problem\n| Student Name | Matriculation Number |\n| --- | --- |\n| Mah Hoy Ping, Kenneth | A0249775J |\n| Yeo Wei Han | A0234774X |\n| Xia Yilin | A0239469L |","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport datetime","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:20:03.403281Z","iopub.execute_input":"2023-11-23T06:20:03.403730Z","iopub.status.idle":"2023-11-23T06:20:03.409326Z","shell.execute_reply.started":"2023-11-23T06:20:03.403692Z","shell.execute_reply":"2023-11-23T06:20:03.408124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading in the dataset","metadata":{}},{"cell_type":"code","source":"# Using the basic stats as outlined in the dataset description (except urls)\nmasterMLpublic = pd.read_csv(\"/kaggle/input/mma-differentials-and-elo/masterMLpublic.csv\",\n                            usecols = [\n                                \"date\", \"result\", \"fighter\", \"opponent\", \"division\",\n                                \"stance\", \"dob\", \"method\", \"total_comp_time\", \"round\", \n                                \"time\", \"referee\", \"time_format\", \"reach\", \"height\",\n                                \"age\", \"knockdowns\", \"sub_attempts\", \"reversals\", \"control\",\n                                \"takedowns_landed\", \"takedowns_attempts\",\n                                \"sig_strikes_landed\", \"sig_strikes_attempts\",\n                                \"total_strikes_landed\", \"total_strikes_attempts\",\n                                \"head_strikes_landed\", \"head_strikes_attempts\",\n                                \"body_strikes_landed\", \"body_strikes_attempts\",\n                                \"leg_strikes_landed\", \"leg_strikes_attempts\",\n                                \"distance_strikes_landed\", \"distance_strikes_attempts\",\n                                \"clinch_strikes_landed\", \"clinch_strikes_attempts\",\n                                \"ground_strikes_landed\", \"ground_strikes_attempts\",\n                                \"KO_losses\", \"days_since_last_comp\", \"lose_streak\",\n                                \"win_streak\", \"win_loss_ratio\", \"total_comp_time\", \"stamina\",\n                                \"num_fights\", \"trueskill\", \"elo\"\n                            ])\n# Convert the date columns from object into datetime\nmasterMLpublic[\"date\"] = pd.to_datetime(masterMLpublic[\"date\"]).dt.date\nmasterMLpublic[\"dob\"] = pd.to_datetime(masterMLpublic[\"dob\"]).dt.date\n# removing draws because nobody wins\nmasterMLpublic = masterMLpublic[masterMLpublic[\"method\"] != \"DRAW\"]\n# removing dqs because they are not standard wins\nmasterMLpublic = masterMLpublic[masterMLpublic[\"method\"] != \"DQ\"]\nmasterMLpublic","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:20:03.431064Z","iopub.execute_input":"2023-11-23T06:20:03.431928Z","iopub.status.idle":"2023-11-23T06:20:58.689051Z","shell.execute_reply.started":"2023-11-23T06:20:03.431882Z","shell.execute_reply":"2023-11-23T06:20:58.687963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masterMLpublic['division'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:20:58.691043Z","iopub.execute_input":"2023-11-23T06:20:58.691410Z","iopub.status.idle":"2023-11-23T06:20:58.707688Z","shell.execute_reply.started":"2023-11-23T06:20:58.691378Z","shell.execute_reply":"2023-11-23T06:20:58.706281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lightweight = masterMLpublic[masterMLpublic[\"division\"] == \"Lightweight\"]\nlightweight","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:20:58.709058Z","iopub.execute_input":"2023-11-23T06:20:58.709832Z","iopub.status.idle":"2023-11-23T06:20:58.758382Z","shell.execute_reply.started":"2023-11-23T06:20:58.709798Z","shell.execute_reply":"2023-11-23T06:20:58.757218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"welterweight = masterMLpublic[masterMLpublic[\"division\"] == \"Welterweight\"]\nwelterweight","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:20:58.762150Z","iopub.execute_input":"2023-11-23T06:20:58.762895Z","iopub.status.idle":"2023-11-23T06:20:58.809569Z","shell.execute_reply.started":"2023-11-23T06:20:58.762846Z","shell.execute_reply":"2023-11-23T06:20:58.808338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming masterMLpublic is a pandas DataFrame\nna_counts = welterweight.isna().sum()\nna_counts","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:20:58.811225Z","iopub.execute_input":"2023-11-23T06:20:58.811550Z","iopub.status.idle":"2023-11-23T06:20:58.824130Z","shell.execute_reply.started":"2023-11-23T06:20:58.811522Z","shell.execute_reply":"2023-11-23T06:20:58.823206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"#null values\naverage_reach_welterweight = welterweight[welterweight['division'] == 'Welterweight']['reach'].mean()\nwelterweight['reach'].fillna(average_reach_welterweight, inplace=True)\n\naverage_age_welterweight = welterweight[welterweight['division'] == 'Welterweight']['age'].mean()\nwelterweight['age'].fillna(average_age_welterweight, inplace=True)\n\naverage_height_welterweight = welterweight[welterweight['division'] == 'Welterweight']['height'].mean()\nwelterweight['height'].fillna(average_height_welterweight, inplace=True)\n\nspecific_value = 216.1080438291005\nwelterweight.loc[welterweight['days_since_last_comp'] == specific_value, 'days_since_last_comp'] = welterweight['age']","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:20:58.825252Z","iopub.execute_input":"2023-11-23T06:20:58.825632Z","iopub.status.idle":"2023-11-23T06:20:58.848354Z","shell.execute_reply.started":"2023-11-23T06:20:58.825601Z","shell.execute_reply":"2023-11-23T06:20:58.847257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_lightweight = lightweight\n\n#null values\naverage_reach_lightweight = cleaned_lightweight[cleaned_lightweight['division'] == 'Lightweight']['reach'].mean()\ncleaned_lightweight['reach'].fillna(average_reach_lightweight, inplace=True)\n\naverage_age_lightweight = cleaned_lightweight[cleaned_lightweight['division'] == 'Lightweight']['age'].mean()\ncleaned_lightweight['age'].fillna(average_age_lightweight, inplace=True)\n\naverage_height_lightweight = cleaned_lightweight[cleaned_lightweight['division'] == 'Lightweight']['height'].mean()\ncleaned_lightweight['height'].fillna(average_height_lightweight, inplace=True)\n\nspecific_value = 216.1080438291005\ncleaned_lightweight.loc[cleaned_lightweight['days_since_last_comp'] == specific_value, 'days_since_last_comp'] = cleaned_lightweight['age']","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:20:58.849742Z","iopub.execute_input":"2023-11-23T06:20:58.850057Z","iopub.status.idle":"2023-11-23T06:20:58.870062Z","shell.execute_reply.started":"2023-11-23T06:20:58.850030Z","shell.execute_reply":"2023-11-23T06:20:58.869079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lightweight = cleaned_lightweight","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:20:58.871512Z","iopub.execute_input":"2023-11-23T06:20:58.871942Z","iopub.status.idle":"2023-11-23T06:20:58.877393Z","shell.execute_reply.started":"2023-11-23T06:20:58.871913Z","shell.execute_reply":"2023-11-23T06:20:58.875824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lightweight","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:20:58.878684Z","iopub.execute_input":"2023-11-23T06:20:58.878995Z","iopub.status.idle":"2023-11-23T06:20:58.921903Z","shell.execute_reply.started":"2023-11-23T06:20:58.878969Z","shell.execute_reply":"2023-11-23T06:20:58.921037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LinUCB ","metadata":{}},{"cell_type":"markdown","source":"### copy and edit/optimise after this markdown","metadata":{}},{"cell_type":"code","source":"lightweight.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:20:58.925219Z","iopub.execute_input":"2023-11-23T06:20:58.925574Z","iopub.status.idle":"2023-11-23T06:20:58.935463Z","shell.execute_reply.started":"2023-11-23T06:20:58.925544Z","shell.execute_reply":"2023-11-23T06:20:58.934088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The new code","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\nclass ContextualThompsonSampling:\n    def __init__(self, n_arms, n_features, delta=0.5,\n                 R=0.01, epsilon=0.5, random_state=456):\n        self.n_arms = n_arms\n        self.n_features = n_features\n        self.random_state = random_state\n\n        # 0 < delta < 1\n        if not isinstance(delta, float):\n            raise ValueError(\"delta should be float\")\n        elif (delta < 0) or (delta >= 1):\n            raise ValueError(\"delta should be in (0, 1]\")\n        else:\n            self.delta = delta\n\n        # R > 0\n        if not isinstance(R, float):\n            raise ValueError(\"R should be float\")\n        elif R <= 0:\n            raise ValueError(\"R should be positive\")\n        else:\n            self.R = R\n\n        # 0 < epsilon < 1\n        if not isinstance(epsilon, float):\n            raise ValueError(\"epsilon should be float\")\n        elif (epsilon < 0) or (epsilon > 1):\n            raise ValueError(\"epsilon should be in (0, 1)\")\n        else:\n            self.epsilon = epsilon\n\n        self.A = [np.identity(n_features) for _ in range(n_arms)]\n        self.b = [np.zeros(n_features) for _ in range(n_arms)]\n\n    def select_arm(self, context):\n        scores = np.zeros(self.n_arms)\n        for arm in range(self.n_arms):\n            A_inv = np.linalg.inv(self.A[arm])\n            mu_hat = A_inv @ self.b[arm]\n            v = self.R * np.sqrt(24 / self.epsilon * self.n_features * np.log(1 / self.delta))\n            mu_tilde = np.random.multivariate_normal(mu_hat.flat, v**2 * A_inv)[..., np.newaxis]\n            scores[arm] = context @ mu_tilde\n\n        selected_arm = np.argmax(scores)\n        return selected_arm\n\n    def update(self, arm, context, reward):\n        self.A[arm] += np.outer(context, context)\n        self.b[arm] += reward * context\n        \n# Assume 'lightweight' is a pandas DataFrame containing your data\n# Assume 'chosen_features' is defined as the list of features for the fighters\nchosen_features = [\n    \"total_comp_time\", \"reach\", \"height\", \"age\",\n    \"knockdowns\", \"sub_attempts\", \"reversals\", \"control\",\n    \"takedowns_landed\", \"takedowns_attempts\",\n    \"sig_strikes_landed\", \"sig_strikes_attempts\",\n    \"total_strikes_landed\", \"total_strikes_attempts\",\n    \"head_strikes_landed\", \"head_strikes_attempts\",\n    \"body_strikes_landed\", \"body_strikes_attempts\",\n    \"leg_strikes_landed\", \"leg_strikes_attempts\",\n    \"distance_strikes_landed\", \"distance_strikes_attempts\",\n    \"clinch_strikes_landed\", \"clinch_strikes_attempts\",\n    \"ground_strikes_landed\", \"ground_strikes_attempts\",\n    \"KO_losses\", \"days_since_last_comp\", \"lose_streak\",\n    \"win_streak\", \"win_loss_ratio\",\n    \"stamina\", \"num_fights\", \"trueskill\", \"elo\"\n]\n\n# Ensure 'result' column exists and indicates the winner (1 for the winner, 0 for the loser)\n\nnum_fights = len(lightweight) // 2\nnum_features = len(chosen_features) * 2  # Features from both fighters\n\n# Initialize variables to track accuracy in chunks\nchunk_size = 100  # Define the size of each chunk for reporting accuracy\nchunk_rewards = [0] * (num_fights // chunk_size + 1)  # List to hold rewards for each chunk\nchunk_correct_predictions = [0] * (num_fights // chunk_size + 1)  # List to hold correct predictions count for each chunk\n\n# Create an instance of the ContextualThompsonSampling class\ncts = ContextualThompsonSampling(2, num_features)\n\ntotal_reward = 0\ncorrect_predictions = 0\n\n# print(\"Timestep | Chosen Arm | Actual Winner | Reward\")\nfor fight_index in range(num_fights):\n    # Get the records for both fighters in the current fight\n    fighter1_record = lightweight.iloc[fight_index * 2]\n    fighter2_record = lightweight.iloc[fight_index * 2 + 1]\n\n    # Convert features from both fighters to numeric types and handle non-numeric entries\n    fighter1_features = pd.to_numeric(fighter1_record[chosen_features], errors='coerce').fillna(0)\n    fighter2_features = pd.to_numeric(fighter2_record[chosen_features], errors='coerce').fillna(0)\n\n    context = np.concatenate([fighter1_features, fighter2_features])\n    \n    # Use Contextual Thompson Sampling to recommend an arm (fighter)\n    chosen_arm = cts.select_arm(context)\n\n    # Determine the actual winner (arm 0 represents fighter 1 and arm 1 represents fighter 2)\n    actual_winner = 0 if fighter1_record['result'] == 1 else 1\n    \n     # Reward is 1 if the chosen arm matches the actual winner, else 0\n    reward = 1 if chosen_arm == actual_winner else 0\n\n    # Update the model\n    cts.update(chosen_arm, context, reward)\n    \n    # print(fight_index)\n    \n    # print(\"Fighter \" + str(fighter1_record['fighter'] + \" \" + str(fighter1_record['result'])) + \", \" + \"Fighter \" + str(fighter2_record['fighter'] + \" \" + str(fighter2_record['result'])))\n    \n    winning_fighter = \"name\"\n    if actual_winner == 0:\n        winning_fighter = fighter1_record['fighter']\n    else:\n        winning_fighter = fighter2_record['fighter']\n        \n    # print(f\"Actual Winner: Fighter \" + winning_fighter)\n    \n    # Determine the current chunk\n    current_chunk = fight_index // chunk_size\n    \n    # Update rewards and correct predictions for the current chunk\n    chunk_rewards[current_chunk] += reward\n    if chosen_arm == actual_winner:\n        chunk_correct_predictions[current_chunk] += 1\n    \n    # Print the timestep, rewards, and chosen arm\n    # print(f\"{fight_index+1:8} | {chosen_arm:11} | {actual_winner:14} | {reward:6}\")\n    \n    # Track total reward and correct predictions\n    total_reward += reward\n    correct_predictions += (chosen_arm == actual_winner)\n\n# Print accuracy for each chunk\nfor i in range(len(chunk_rewards)):\n    if (i + 1) * chunk_size <= num_fights:\n        print(f\"Accuracy for steps {i * chunk_size + 1} - {(i + 1) * chunk_size}: {chunk_correct_predictions[i] / chunk_size}\")\n    else:  # Handle the last chunk which might be smaller than chunk_size\n        print(f\"Accuracy for steps {i * chunk_size + 1} - {num_fights}: {chunk_correct_predictions[i] / (num_fights - i * chunk_size)}\")\n        \nimport matplotlib.pyplot as plt\n\n# Calculate the accuracy for each chunk\naccuracies = [chunk_correct_predictions[i] / chunk_size if (i + 1) * chunk_size <= num_fights\n              else chunk_correct_predictions[i] / (num_fights - i * chunk_size)\n              for i in range(len(chunk_rewards))]\n\n# Create x-axis values for the plot\nx_values = [i * chunk_size + 1 if (i + 1) * chunk_size <= num_fights\n            else num_fights\n            for i in range(len(chunk_rewards))]\n\n# Plot the line graph\nplt.plot(x_values, accuracies, marker='o', linestyle='-')\nplt.title('Accuracy Over Chunks')\nplt.xlabel('Steps')\nplt.ylabel('Accuracy')\nplt.grid(True)\nplt.show()\n        \n# Calculate accuracy\naccuracy = correct_predictions / num_fights\nprint(f\"Total reward: {total_reward}\")\nprint(f\"Accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:20:58.937516Z","iopub.execute_input":"2023-11-23T06:20:58.938388Z","iopub.status.idle":"2023-11-23T06:21:14.752087Z","shell.execute_reply.started":"2023-11-23T06:20:58.938338Z","shell.execute_reply":"2023-11-23T06:21:14.751074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Iterate through the 'alpha_values' value to find the best","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nclass ModifiedWinLossLinUCB:\n    def __init__(self, num_arms, num_features, alpha=0.1):\n        self.num_arms = num_arms  # This is actually the number of fights, not fighters\n        self.num_features = num_features\n        self.alpha = alpha\n        self.A = [np.identity(num_features) for _ in range(self.num_arms)]  # One A matrix per fight\n        self.b = [np.zeros((num_features, 1)) for _ in range(self.num_arms)]  # One b vector per fight\n\n    def recommend(self, context):\n        chosen_arm = None\n        max_ucb = float('-inf')\n\n        for arm in [0, 1]:  # Two possible arms: fighter 1 wins or fighter 2 wins\n            x = context.reshape((self.num_features, 1))\n            A_inv = np.linalg.inv(self.A[arm])\n            theta = np.dot(A_inv, self.b[arm])\n            ucb_value = np.dot(x.T, np.dot(A_inv, x))\n            ucb = np.dot(theta.T, x) + self.alpha * np.sqrt(ucb_value.item())\n\n            if ucb > max_ucb:\n                max_ucb = ucb\n                chosen_arm = arm\n\n        return chosen_arm\n\n\n    def update(self, fight_index, context, reward):\n        x = context.reshape((self.num_features, 1))\n        self.A[fight_index] += np.dot(x, x.T)\n        self.b[fight_index] += reward * x\n        \nchosen_features = [\n    \"total_comp_time\", \"reach\", \"height\", \"age\",\n    \"knockdowns\", \"sub_attempts\", \"reversals\", \"control\",\n    \"takedowns_landed\", \"takedowns_attempts\",\n    \"sig_strikes_landed\", \"sig_strikes_attempts\",\n    \"total_strikes_landed\", \"total_strikes_attempts\",\n    \"head_strikes_landed\", \"head_strikes_attempts\",\n    \"body_strikes_landed\", \"body_strikes_attempts\",\n    \"leg_strikes_landed\", \"leg_strikes_attempts\",\n    \"distance_strikes_landed\", \"distance_strikes_attempts\",\n    \"clinch_strikes_landed\", \"clinch_strikes_attempts\",\n    \"ground_strikes_landed\", \"ground_strikes_attempts\",\n    \"KO_losses\", \"days_since_last_comp\", \"lose_streak\",\n    \"win_streak\", \"win_loss_ratio\",\n    \"stamina\", \"num_fights\", \"trueskill\", \"elo\"\n]\n\nnum_fights = len(lightweight) // 2\nnum_features = len(chosen_features) * 2  # Features from both fighters\n\n#track accuracy in chunks\nchunk_size = 100  # Define the size of each chunk\nchunk_rewards = [0] * (num_fights // chunk_size + 1)  # List to hold rewards for each chunk\nchunk_correct_predictions = [0] * (num_fights // chunk_size + 1)  # List to hold correct predictions count for each chunk\n\nbandit = ModifiedWinLossLinUCB(num_fights, num_features)\n\n# List to store total accuracies for each epsilon value\ntotal_accuracies = []\n\ntotal_reward = 0\ncorrect_predictions = 0\nalpha_values = [0.01, 0.1, 0.5, 1.0, 2.0]  # Add more alpha values as needed\n\nfor alpha in alpha_values:\n    print(f\"\\nRunning with alpha = {alpha}\")\n\n    # Initialize the bandit with the current alpha\n    bandit = ModifiedWinLossLinUCB(num_fights, num_features, alpha)\n\n    total_reward = 0\n    correct_predictions = 0\n\n    chunk_rewards = [0] * (num_fights // chunk_size + 1)\n    chunk_correct_predictions = [0] * (num_fights // chunk_size + 1)\n    \n    # print(\"Timestep | Chosen Arm | Actual Winner | Reward\")\n    for fight_index in range(num_fights):\n        # Get the records for both fighters in the current fight\n        fighter1_record = lightweight.iloc[fight_index * 2]\n        fighter2_record = lightweight.iloc[fight_index * 2 + 1]\n\n        # ensure features are numeric types\n        fighter1_features = pd.to_numeric(fighter1_record[chosen_features], errors='coerce').fillna(0)\n        fighter2_features = pd.to_numeric(fighter2_record[chosen_features], errors='coerce').fillna(0)\n\n        context = np.concatenate([fighter1_features, fighter2_features])\n        # Predict the winner using the combined context\n        chosen_arm = bandit.recommend(context)  # 0 or 1\n\n        # Determine the actual winner (arm 0 represents fighter 1 and arm 1 represents fighter 2)\n        actual_winner = 0 if fighter1_record['result'] == 1 else 1\n\n        # print(fight_index)\n        # print(\"Fighter \" + str(fighter1_record['fighter'] + \" \" + str(fighter1_record['result'])) + \", \" + \"Fighter \" + str(fighter2_record['fighter'] + \" \" + str(fighter2_record['result'])))\n\n        winning_fighter = \"name\"\n        if actual_winner == 0:\n            winning_fighter = fighter1_record['fighter']\n        else:\n            winning_fighter = fighter2_record['fighter']\n        # print(f\"Actual Winner: Fighter \" + winning_fighter)\n\n        # Update the bandit with the outcome of the fight\n        reward = 1 if chosen_arm == actual_winner else 0\n        bandit.update(fight_index, context, reward)\n\n        # Determine the current chunk\n        current_chunk = fight_index // chunk_size\n\n        # Update rewards and correct predictions for the current chunk\n        chunk_rewards[current_chunk] += reward\n        if chosen_arm == actual_winner:\n            chunk_correct_predictions[current_chunk] += 1\n\n        # Print\n        # print(f\"{fight_index+1:8} | {chosen_arm:11} | {actual_winner:14} | {reward:6}\")\n\n        # Track total reward and correct predictions\n        total_reward += reward\n        correct_predictions += (chosen_arm == actual_winner)\n        \n    # Calculate accuracy\n    accuracy = correct_predictions / num_fights\n\n    # Append accuracy to the list\n    total_accuracies.append(accuracy)\n\n    # Print accuracy for each chunk\n    for i in range(len(chunk_rewards)):\n        if (i + 1) * chunk_size <= num_fights:\n            print(f\"Accuracy for steps {i * chunk_size + 1} - {(i + 1) * chunk_size}: {chunk_correct_predictions[i] / chunk_size}\")\n        else:  # Handle the last chunk\n            print(f\"Accuracy for steps {i * chunk_size + 1} - {num_fights}: {chunk_correct_predictions[i] / (num_fights - i * chunk_size)}\")\n\n    import matplotlib.pyplot as plt\n\n    # Calculate the accuracy for each chunk\n    accuracies = [chunk_correct_predictions[i] / chunk_size if (i + 1) * chunk_size <= num_fights\n                  else chunk_correct_predictions[i] / (num_fights - i * chunk_size)\n                  for i in range(len(chunk_rewards))]\n\n    # Create x-axis values for the plot\n    x_values = [i * chunk_size + 1 if (i + 1) * chunk_size <= num_fights\n                else num_fights\n                for i in range(len(chunk_rewards))]\n\n    # Plot the line graph\n    plt.plot(x_values, accuracies, marker='o', linestyle='-')\n    plt.title('Accuracy Over Chunks')\n    plt.xlabel('Steps')\n    plt.ylabel('Accuracy')\n    plt.grid(True)\n    plt.show()\n\n    # Calculate accuracy\n    accuracy = correct_predictions / num_fights\n    print(f\"Total reward: {total_reward}\")\n    print(f\"Accuracy: {accuracy}\")\n    \n# Plot total accuracies for each epsilon value\nplt.figure(figsize=(10, 6))\nplt.plot(epsilon_values, total_accuracies, marker='o', linestyle='-')\nplt.title('Total Accuracy for Different Alpha Values')\nplt.xlabel('Alpha')\nplt.ylabel('Total Accuracy')\nplt.grid(True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:21:14.753754Z","iopub.execute_input":"2023-11-23T06:21:14.754604Z","iopub.status.idle":"2023-11-23T06:21:48.232322Z","shell.execute_reply.started":"2023-11-23T06:21:14.754567Z","shell.execute_reply":"2023-11-23T06:21:48.231047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Contextual thompson sampling","metadata":{}},{"cell_type":"markdown","source":"## Iterate through the 'delta_values' value to find the best","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\nclass ContextualThompsonSampling:\n    def __init__(self, n_arms, n_features, delta=0.5,\n                 R=0.01, epsilon=0.5, random_state=456):\n        self.n_arms = n_arms\n        self.n_features = n_features\n        self.random_state = random_state\n\n        # 0 < delta < 1\n        if not isinstance(delta, float):\n            raise ValueError(\"delta should be float\")\n        elif (delta < 0) or (delta >= 1):\n            raise ValueError(\"delta should be in (0, 1]\")\n        else:\n            self.delta = delta\n\n        # R > 0\n        if not isinstance(R, float):\n            raise ValueError(\"R should be float\")\n        elif R <= 0:\n            raise ValueError(\"R should be positive\")\n        else:\n            self.R = R\n\n        # 0 < epsilon < 1\n        if not isinstance(epsilon, float):\n            raise ValueError(\"epsilon should be float\")\n        elif (epsilon < 0) or (epsilon > 1):\n            raise ValueError(\"epsilon should be in (0, 1)\")\n        else:\n            self.epsilon = epsilon\n\n        self.A = [np.identity(n_features) for _ in range(n_arms)]\n        self.b = [np.zeros(n_features) for _ in range(n_arms)]\n\n    def select_arm(self, context):\n        scores = np.zeros(self.n_arms)\n        for arm in range(self.n_arms):\n            A_inv = np.linalg.inv(self.A[arm])\n            mu_hat = A_inv @ self.b[arm]\n            v = self.R * np.sqrt(24 / self.epsilon * self.n_features * np.log(1 / self.delta))\n            mu_tilde = np.random.multivariate_normal(mu_hat.flat, v**2 * A_inv)[..., np.newaxis]\n            scores[arm] = context @ mu_tilde\n\n        selected_arm = np.argmax(scores)\n        return selected_arm\n\n    def update(self, arm, context, reward):\n        self.A[arm] += np.outer(context, context)\n        self.b[arm] += reward * context\n        \n# Assume 'lightweight' is a pandas DataFrame containing your data\n# Assume 'chosen_features' is defined as the list of features for the fighters\nchosen_features = [\n    \"total_comp_time\", \"reach\", \"height\", \"age\",\n    \"knockdowns\", \"sub_attempts\", \"reversals\", \"control\",\n    \"takedowns_landed\", \"takedowns_attempts\",\n    \"sig_strikes_landed\", \"sig_strikes_attempts\",\n    \"total_strikes_landed\", \"total_strikes_attempts\",\n    \"head_strikes_landed\", \"head_strikes_attempts\",\n    \"body_strikes_landed\", \"body_strikes_attempts\",\n    \"leg_strikes_landed\", \"leg_strikes_attempts\",\n    \"distance_strikes_landed\", \"distance_strikes_attempts\",\n    \"clinch_strikes_landed\", \"clinch_strikes_attempts\",\n    \"ground_strikes_landed\", \"ground_strikes_attempts\",\n    \"KO_losses\", \"days_since_last_comp\", \"lose_streak\",\n    \"win_streak\", \"win_loss_ratio\",\n    \"stamina\", \"num_fights\", \"trueskill\", \"elo\"\n]\n# Define a list of delta values to iterate over\ndelta_values = [0.1, 0.2, 0.4, 0.6, 0.8, 0.999]\n\n# Ensure 'result' column exists and indicates the winner (1 for the winner, 0 for the loser)\nnum_fights = len(lightweight) // 2\nnum_features = len(chosen_features) * 2  # Features from both fighters\n\n# Initialize variables to track accuracy in chunks\nchunk_size = 100  # Define the size of each chunk for reporting accuracy\nchunk_rewards = [0] * (num_fights // chunk_size + 1)  # List to hold rewards for each chunk\nchunk_correct_predictions = [0] * (num_fights // chunk_size + 1)  # List to hold correct predictions count for each chunk\n\n# List to store total accuracies for each epsilon value\ntotal_accuracies = []\n\n# Iterate over delta values\nfor delta_value in delta_values:\n    # Create an instance of the ContextualThompsonSampling class with the current delta value\n    cts = ContextualThompsonSampling(2, num_features, delta=delta_value)\n\n    # Initialize variables to track accuracy in chunks for the current delta\n    chunk_rewards = [0] * (num_fights // chunk_size + 1)\n    chunk_correct_predictions = [0] * (num_fights // chunk_size + 1)\n    total_reward = 0\n    correct_predictions = 0\n\n    # print(\"Timestep | Chosen Arm | Actual Winner | Reward\")\n    for fight_index in range(num_fights):\n        # Get the records for both fighters in the current fight\n        fighter1_record = lightweight.iloc[fight_index * 2]\n        fighter2_record = lightweight.iloc[fight_index * 2 + 1]\n\n        # Convert features from both fighters to numeric types and handle non-numeric entries\n        fighter1_features = pd.to_numeric(fighter1_record[chosen_features], errors='coerce').fillna(0)\n        fighter2_features = pd.to_numeric(fighter2_record[chosen_features], errors='coerce').fillna(0)\n\n        context = np.concatenate([fighter1_features, fighter2_features])\n\n        # Use Contextual Thompson Sampling to recommend an arm (fighter)\n        chosen_arm = cts.select_arm(context)\n\n        # Determine the actual winner (arm 0 represents fighter 1 and arm 1 represents fighter 2)\n        actual_winner = 0 if fighter1_record['result'] == 1 else 1\n\n         # Reward is 1 if the chosen arm matches the actual winner, else 0\n        reward = 1 if chosen_arm == actual_winner else 0\n\n        # Update the model\n        cts.update(chosen_arm, context, reward)\n\n        # print(fight_index)\n\n        # print(\"Fighter \" + str(fighter1_record['fighter'] + \" \" + str(fighter1_record['result'])) + \", \" + \"Fighter \" + str(fighter2_record['fighter'] + \" \" + str(fighter2_record['result'])))\n\n        winning_fighter = \"name\"\n        if actual_winner == 0:\n            winning_fighter = fighter1_record['fighter']\n        else:\n            winning_fighter = fighter2_record['fighter']\n\n        # print(f\"Actual Winner: Fighter \" + winning_fighter)\n\n        # Determine the current chunk\n        current_chunk = fight_index // chunk_size\n\n        # Update rewards and correct predictions for the current chunk\n        chunk_rewards[current_chunk] += reward\n        if chosen_arm == actual_winner:\n            chunk_correct_predictions[current_chunk] += 1\n\n        # Print the timestep, rewards, and chosen arm\n        # print(f\"{fight_index+1:8} | {chosen_arm:11} | {actual_winner:14} | {reward:6}\")\n\n        # Track total reward and correct predictions\n        total_reward += reward\n        correct_predictions += (chosen_arm == actual_winner)\n        \n    # Calculate accuracy\n    accuracy = correct_predictions / num_fights\n\n    # Append accuracy to the list\n    total_accuracies.append(accuracy)\n    \n    # Print accuracy for each chunk\n    for i in range(len(chunk_rewards)):\n        if (i + 1) * chunk_size <= num_fights:\n            print(f\"Accuracy for steps {i * chunk_size + 1} - {(i + 1) * chunk_size}: {chunk_correct_predictions[i] / chunk_size}\")\n        else:  # Handle the last chunk which might be smaller than chunk_size\n            print(f\"Accuracy for steps {i * chunk_size + 1} - {num_fights}: {chunk_correct_predictions[i] / (num_fights - i * chunk_size)}\")\n\n    # Calculate accuracy\n    accuracy = correct_predictions / num_fights\n    print(f\"Total reward for delta={delta_value}: {total_reward}\")\n    print(f\"Accuracy for delta={delta_value}: {accuracy}\") # Plotting (optional)\n    x_values = [i * chunk_size + 1 if (i + 1) * chunk_size <= num_fights\n                else num_fights\n                for i in range(len(chunk_rewards))]\n    accuracies = [chunk_correct_predictions[i] / chunk_size if (i + 1) * chunk_size <= num_fights\n                  else chunk_correct_predictions[i] / (num_fights - i * chunk_size)\n                  for i in range(len(chunk_rewards))]\n    plt.plot(x_values, accuracies, marker='o', linestyle='-', label=f'delta={delta_value}')\n    # Show the plot\n    plt.title('Accuracy Over Chunks')\n    plt.xlabel('Steps')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n    \n# Plot total accuracies for each epsilon value\nplt.figure(figsize=(10, 6))\nplt.plot(delta_values, total_accuracies, marker='o', linestyle='-')\nplt.title('Total Accuracy for Different Delta Values')\nplt.xlabel('Delta')\nplt.ylabel('Total Accuracy')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:21:48.234034Z","iopub.execute_input":"2023-11-23T06:21:48.234427Z","iopub.status.idle":"2023-11-23T06:23:18.159363Z","shell.execute_reply.started":"2023-11-23T06:21:48.234393Z","shell.execute_reply":"2023-11-23T06:23:18.158249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Iterate through the 'epsilon_values' value to find the best","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\nclass ContextualThompsonSampling:\n    def __init__(self, n_arms, n_features, delta=0.5,\n                 R=0.01, epsilon=0.5, random_state=456):\n        self.n_arms = n_arms\n        self.n_features = n_features\n        self.random_state = random_state\n\n        # 0 < delta < 1\n        if not isinstance(delta, float):\n            raise ValueError(\"delta should be float\")\n        elif (delta < 0) or (delta >= 1):\n            raise ValueError(\"delta should be in (0, 1]\")\n        else:\n            self.delta = delta\n\n        # R > 0\n        if not isinstance(R, float):\n            raise ValueError(\"R should be float\")\n        elif R <= 0:\n            raise ValueError(\"R should be positive\")\n        else:\n            self.R = R\n\n        # 0 < epsilon < 1\n        if not isinstance(epsilon, float):\n            raise ValueError(\"epsilon should be float\")\n        elif (epsilon < 0) or (epsilon > 1):\n            raise ValueError(\"epsilon should be in (0, 1)\")\n        else:\n            self.epsilon = epsilon\n\n        self.A = [np.identity(n_features) for _ in range(n_arms)]\n        self.b = [np.zeros(n_features) for _ in range(n_arms)]\n\n    def select_arm(self, context):\n        scores = np.zeros(self.n_arms)\n        for arm in range(self.n_arms):\n            A_inv = np.linalg.inv(self.A[arm])\n            mu_hat = A_inv @ self.b[arm]\n            v = self.R * np.sqrt(24 / self.epsilon * self.n_features * np.log(1 / self.delta))\n            mu_tilde = np.random.multivariate_normal(mu_hat.flat, v**2 * A_inv)[..., np.newaxis]\n            scores[arm] = context @ mu_tilde\n\n        selected_arm = np.argmax(scores)\n        return selected_arm\n\n    def update(self, arm, context, reward):\n        self.A[arm] += np.outer(context, context)\n        self.b[arm] += reward * context\n        \n# Assume 'lightweight' is a pandas DataFrame containing your data\n# Assume 'chosen_features' is defined as the list of features for the fighters\nchosen_features = [\n    \"total_comp_time\", \"reach\", \"height\", \"age\",\n    \"knockdowns\", \"sub_attempts\", \"reversals\", \"control\",\n    \"takedowns_landed\", \"takedowns_attempts\",\n    \"sig_strikes_landed\", \"sig_strikes_attempts\",\n    \"total_strikes_landed\", \"total_strikes_attempts\",\n    \"head_strikes_landed\", \"head_strikes_attempts\",\n    \"body_strikes_landed\", \"body_strikes_attempts\",\n    \"leg_strikes_landed\", \"leg_strikes_attempts\",\n    \"distance_strikes_landed\", \"distance_strikes_attempts\",\n    \"clinch_strikes_landed\", \"clinch_strikes_attempts\",\n    \"ground_strikes_landed\", \"ground_strikes_attempts\",\n    \"KO_losses\", \"days_since_last_comp\", \"lose_streak\",\n    \"win_streak\", \"win_loss_ratio\",\n    \"stamina\", \"num_fights\", \"trueskill\", \"elo\"\n]\n\n# List of epsilon values to iterate over\nepsilon_values = [0.1, 0.2, 0.3, 0.4, 0.5]\n\n# Ensure 'result' column exists and indicates the winner (1 for the winner, 0 for the loser)\nnum_fights = len(lightweight) // 2\nnum_features = len(chosen_features) * 2  # Features from both fighters\n\n# Initialize variables to track accuracy in chunks\nchunk_size = 100  # Define the size of each chunk for reporting accuracy\nchunk_rewards = [0] * (num_fights // chunk_size + 1)  # List to hold rewards for each chunk\nchunk_correct_predictions = [0] * (num_fights // chunk_size + 1)  # List to hold correct predictions count for each chunk\n\n# List to store total accuracies for each epsilon value\ntotal_accuracies = []\n\n# Iterate over epsilon values\nfor epsilon in epsilon_values:\n    # Create an instance of the ContextualThompsonSampling class with the current epsilon\n    cts = ContextualThompsonSampling(2, num_features, epsilon=epsilon)\n\n    # Initialize variables to track accuracy in chunks for the current delta\n    chunk_rewards = [0] * (num_fights // chunk_size + 1)\n    chunk_correct_predictions = [0] * (num_fights // chunk_size + 1)\n    total_reward = 0\n    correct_predictions = 0\n\n    # print(\"Timestep | Chosen Arm | Actual Winner | Reward\")\n    for fight_index in range(num_fights):\n        # Get the records for both fighters in the current fight\n        fighter1_record = lightweight.iloc[fight_index * 2]\n        fighter2_record = lightweight.iloc[fight_index * 2 + 1]\n\n        # Convert features from both fighters to numeric types and handle non-numeric entries\n        fighter1_features = pd.to_numeric(fighter1_record[chosen_features], errors='coerce').fillna(0)\n        fighter2_features = pd.to_numeric(fighter2_record[chosen_features], errors='coerce').fillna(0)\n\n        context = np.concatenate([fighter1_features, fighter2_features])\n\n        # Use Contextual Thompson Sampling to recommend an arm (fighter)\n        chosen_arm = cts.select_arm(context)\n\n        # Determine the actual winner (arm 0 represents fighter 1 and arm 1 represents fighter 2)\n        actual_winner = 0 if fighter1_record['result'] == 1 else 1\n\n         # Reward is 1 if the chosen arm matches the actual winner, else 0\n        reward = 1 if chosen_arm == actual_winner else 0\n\n        # Update the model\n        cts.update(chosen_arm, context, reward)\n\n        # print(fight_index)\n\n        # print(\"Fighter \" + str(fighter1_record['fighter'] + \" \" + str(fighter1_record['result'])) + \", \" + \"Fighter \" + str(fighter2_record['fighter'] + \" \" + str(fighter2_record['result'])))\n\n        winning_fighter = \"name\"\n        if actual_winner == 0:\n            winning_fighter = fighter1_record['fighter']\n        else:\n            winning_fighter = fighter2_record['fighter']\n\n        # print(f\"Actual Winner: Fighter \" + winning_fighter)\n\n        # Determine the current chunk\n        current_chunk = fight_index // chunk_size\n\n        # Update rewards and correct predictions for the current chunk\n        chunk_rewards[current_chunk] += reward\n        if chosen_arm == actual_winner:\n            chunk_correct_predictions[current_chunk] += 1\n\n        # Print the timestep, rewards, and chosen arm\n        # print(f\"{fight_index+1:8} | {chosen_arm:11} | {actual_winner:14} | {reward:6}\")\n\n        # Track total reward and correct predictions\n        total_reward += reward\n        correct_predictions += (chosen_arm == actual_winner)\n        \n    # Calculate accuracy\n    accuracy = correct_predictions / num_fights\n\n    # Append accuracy to the list\n    total_accuracies.append(accuracy)\n    \n    # Print accuracy for each chunk\n    for i in range(len(chunk_rewards)):\n        if (i + 1) * chunk_size <= num_fights:\n            print(f\"Epsilon={epsilon}: Accuracy for steps {i * chunk_size + 1} - {(i + 1) * chunk_size}: {accuracies[i]}\")\n        else:\n            print(f\"Epsilon={epsilon}: Accuracy for steps {i * chunk_size + 1} - {num_fights}: {accuracies[i]}\")\n\n    # Calculate accuracy\n    accuracy = correct_predictions / num_fights\n    print(f\"Total reward for epsilon={epsilon}: {total_reward}\")\n    print(f\"Accuracy for epsilon={epsilon}: {accuracy}\")\n\n    x_values = [i * chunk_size + 1 if (i + 1) * chunk_size <= num_fights\n                else num_fights\n                for i in range(len(chunk_rewards))]\n    accuracies = [chunk_correct_predictions[i] / chunk_size if (i + 1) * chunk_size <= num_fights\n                  else chunk_correct_predictions[i] / (num_fights - i * chunk_size)\n                  for i in range(len(chunk_rewards))]\n    plt.plot(x_values, accuracies, marker='o', linestyle='-', label=f'epsilon={epsilon}')\n    # Show the plot\n    plt.title('Accuracy Over Chunks')\n    plt.xlabel('Steps')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n    total_reward = 0\n    correct_predictions = 0\n# Plot total accuracies for each epsilon value\nplt.figure(figsize=(10, 6))\nplt.plot(epsilon_values, total_accuracies, marker='o', linestyle='-')\nplt.title('Total Accuracy for Different Epsilon Values')\nplt.xlabel('Epsilon')\nplt.ylabel('Total Accuracy')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:23:18.161609Z","iopub.execute_input":"2023-11-23T06:23:18.162395Z","iopub.status.idle":"2023-11-23T06:24:35.581420Z","shell.execute_reply.started":"2023-11-23T06:23:18.162357Z","shell.execute_reply":"2023-11-23T06:24:35.580207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Iterate through the 'R' value to find the best","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\nclass ContextualThompsonSampling:\n    def __init__(self, n_arms, n_features, delta=0.5,\n                 R=0.01, epsilon=0.5, random_state=456):\n        self.n_arms = n_arms\n        self.n_features = n_features\n        self.random_state = random_state\n\n        # 0 < delta < 1\n        if not isinstance(delta, float):\n            raise ValueError(\"delta should be float\")\n        elif (delta < 0) or (delta >= 1):\n            raise ValueError(\"delta should be in (0, 1]\")\n        else:\n            self.delta = delta\n\n        # R > 0\n        if not isinstance(R, float):\n            raise ValueError(\"R should be float\")\n        elif R <= 0:\n            raise ValueError(\"R should be positive\")\n        else:\n            self.R = R\n\n        # 0 < epsilon < 1\n        if not isinstance(epsilon, float):\n            raise ValueError(\"epsilon should be float\")\n        elif (epsilon < 0) or (epsilon > 1):\n            raise ValueError(\"epsilon should be in (0, 1)\")\n        else:\n            self.epsilon = epsilon\n\n        self.A = [np.identity(n_features) for _ in range(n_arms)]\n        self.b = [np.zeros(n_features) for _ in range(n_arms)]\n\n    def select_arm(self, context):\n        scores = np.zeros(self.n_arms)\n        for arm in range(self.n_arms):\n            A_inv = np.linalg.inv(self.A[arm])\n            mu_hat = A_inv @ self.b[arm]\n            v = self.R * np.sqrt(24 / self.epsilon * self.n_features * np.log(1 / self.delta))\n\n            # Add a small regularization term to ensure positive-semidefinite covariance matrix\n            regularization_term = 1e-6\n            cov_matrix = v**2 * A_inv + regularization_term * np.identity(self.n_features)\n\n            # Ensure the covariance matrix is symmetric\n            cov_matrix = 0.5 * (cov_matrix + cov_matrix.T)\n\n            mu_tilde = np.random.multivariate_normal(mu_hat.flat, cov_matrix)[..., np.newaxis]\n            scores[arm] = context @ mu_tilde\n\n        selected_arm = np.argmax(scores)\n        return selected_arm\n\n    def update(self, arm, context, reward):\n        self.A[arm] += np.outer(context, context)\n        self.b[arm] += reward * context\n        \n        \nchosen_features = [\n    \"total_comp_time\", \"reach\", \"height\", \"age\",\n    \"knockdowns\", \"sub_attempts\", \"reversals\", \"control\",\n    \"takedowns_landed\", \"takedowns_attempts\",\n    \"sig_strikes_landed\", \"sig_strikes_attempts\",\n    \"total_strikes_landed\", \"total_strikes_attempts\",\n    \"head_strikes_landed\", \"head_strikes_attempts\",\n    \"body_strikes_landed\", \"body_strikes_attempts\",\n    \"leg_strikes_landed\", \"leg_strikes_attempts\",\n    \"distance_strikes_landed\", \"distance_strikes_attempts\",\n    \"clinch_strikes_landed\", \"clinch_strikes_attempts\",\n    \"ground_strikes_landed\", \"ground_strikes_attempts\",\n    \"KO_losses\", \"days_since_last_comp\", \"lose_streak\",\n    \"win_streak\", \"win_loss_ratio\",\n    \"stamina\", \"num_fights\", \"trueskill\", \"elo\"\n]\n\n# Iterate over different values of 'R'\nR_values = [0.01, 0.1, 0.5, 1.0]  \n\nnum_fights = len(lightweight) // 2\nnum_features = len(chosen_features) * 2  # Features from both fighters\n\n# Initialize variables to track accuracy in chunks\nchunk_size = 100  # Define the size of each chunk for reporting accuracy\nchunk_rewards = [0] * (num_fights // chunk_size + 1)  # List to hold rewards for each chunk\nchunk_correct_predictions = [0] * (num_fights // chunk_size + 1)  # List to hold correct predictions count for each chunk\n\n# List to store total accuracies for each epsilon value\ntotal_accuracies = []\n\n# Iterate over epsilon values\nfor R_value in R_values:\n    cts = ContextualThompsonSampling(2, num_features, R=R_value)\n\n    total_reward = 0\n    correct_predictions = 0\n    chunk_rewards = [0] * (num_fights // chunk_size + 1)\n    chunk_correct_predictions = [0] * (num_fights // chunk_size + 1)\n\n    for fight_index in range(num_fights):\n        # Get the records for both fighters in the current fight\n        fighter1_record = lightweight.iloc[fight_index * 2]\n        fighter2_record = lightweight.iloc[fight_index * 2 + 1]\n\n        # Convert features from both fighters to numeric types and handle non-numeric entries\n        fighter1_features = pd.to_numeric(fighter1_record[chosen_features], errors='coerce').fillna(0)\n        fighter2_features = pd.to_numeric(fighter2_record[chosen_features], errors='coerce').fillna(0)\n\n        context = np.concatenate([fighter1_features, fighter2_features])\n\n        # Use Contextual Thompson Sampling to recommend an arm (fighter)\n        chosen_arm = cts.select_arm(context)\n\n        # Determine the actual winner (arm 0 represents fighter 1 and arm 1 represents fighter 2)\n        actual_winner = 0 if fighter1_record['result'] == 1 else 1\n\n         # Reward is 1 if the chosen arm matches the actual winner, else 0\n        reward = 1 if chosen_arm == actual_winner else 0\n\n        # Update the model\n        cts.update(chosen_arm, context, reward)\n\n        winning_fighter = \"name\"\n        if actual_winner == 0:\n            winning_fighter = fighter1_record['fighter']\n        else:\n            winning_fighter = fighter2_record['fighter']\n\n        # Determine the current chunk\n        current_chunk = fight_index // chunk_size\n\n        # Update rewards and correct predictions for the current chunk\n        chunk_rewards[current_chunk] += reward\n        if chosen_arm == actual_winner:\n            chunk_correct_predictions[current_chunk] += 1\n\n        # Track total reward and correct predictions\n        total_reward += reward\n        correct_predictions += (chosen_arm == actual_winner)\n        \n    # Calculate accuracy\n    accuracy = correct_predictions / num_fights\n\n    # Append accuracy to the list\n    total_accuracies.append(accuracy)\n    \n    # Print accuracy for each chunk\n    for i in range(len(chunk_rewards)):\n        if (i + 1) * chunk_size <= num_fights:\n            print(f\"R_value={R_value}: Accuracy for steps {i * chunk_size + 1} - {(i + 1) * chunk_size}: {accuracies[i]}\")\n        else:\n            print(f\"R_value={R_value}: Accuracy for steps {i * chunk_size + 1} - {num_fights}: {accuracies[i]}\")\n\n    # Calculate accuracy\n    accuracy = correct_predictions / num_fights\n    print(f\"Total reward for 'r'={R_value}: {total_reward}\")\n    print(f\"Accuracy for 'r'={R_value}: {accuracy}\")\n\n    # Create x-axis values for the plot\n    x_values = [i * chunk_size + 1 if (i + 1) * chunk_size <= num_fights\n                else num_fights\n                for i in range(len(chunk_rewards))]\n    accuracies = [chunk_correct_predictions[i] / chunk_size if (i + 1) * chunk_size <= num_fights\n                  else chunk_correct_predictions[i] / (num_fights - i * chunk_size)\n                  for i in range(len(chunk_rewards))]\n    plt.plot(x_values, accuracies, marker='o', linestyle='-', label=f'R={R_value}')\n    # Show the plot\n    plt.title('Accuracy Over Chunks')\n    plt.xlabel('Steps')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n    total_reward = 0\n    correct_predictions = 0\n    \n# Plot total accuracies for each epsilon value\nplt.figure(figsize=(10, 6))\nplt.plot(R_values, total_accuracies, marker='o', linestyle='-')  # Use R_values instead of epsilon_values\nplt.title(\"Total Accuracy for Different 'R' Values\")\nplt.xlabel('R value')\nplt.ylabel('Total Accuracy')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:24:35.583083Z","iopub.execute_input":"2023-11-23T06:24:35.583888Z","iopub.status.idle":"2023-11-23T06:25:37.599409Z","shell.execute_reply.started":"2023-11-23T06:24:35.583854Z","shell.execute_reply":"2023-11-23T06:25:37.597913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimised model \n### most optimised delta, epsilon and r is ,0.9 , 0.40, 0.10 respectively","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\nclass ContextualThompsonSampling:\n    def __init__(self, n_arms, n_features, delta=0.9,\n                 R=0.1, epsilon=0.4, random_state=456):\n        self.n_arms = n_arms\n        self.n_features = n_features\n        self.random_state = random_state\n\n        # 0 < delta < 1\n        if not isinstance(delta, float):\n            raise ValueError(\"delta should be float\")\n        elif (delta < 0) or (delta >= 1):\n            raise ValueError(\"delta should be in (0, 1]\")\n        else:\n            self.delta = delta\n\n        # R > 0\n        if not isinstance(R, float):\n            raise ValueError(\"R should be float\")\n        elif R <= 0:\n            raise ValueError(\"R should be positive\")\n        else:\n            self.R = R\n\n        # 0 < epsilon < 1\n        if not isinstance(epsilon, float):\n            raise ValueError(\"epsilon should be float\")\n        elif (epsilon < 0) or (epsilon > 1):\n            raise ValueError(\"epsilon should be in (0, 1)\")\n        else:\n            self.epsilon = epsilon\n\n        self.A = [np.identity(n_features) for _ in range(n_arms)]\n        self.b = [np.zeros(n_features) for _ in range(n_arms)]\n\n    def select_arm(self, context):\n        scores = np.zeros(self.n_arms)\n        for arm in range(self.n_arms):\n            A_inv = np.linalg.inv(self.A[arm])\n            mu_hat = A_inv @ self.b[arm]\n            v = self.R * np.sqrt(24 / self.epsilon * self.n_features * np.log(1 / self.delta))\n            mu_tilde = np.random.multivariate_normal(mu_hat.flat, v**2 * A_inv)[..., np.newaxis]\n            scores[arm] = context @ mu_tilde\n\n        selected_arm = np.argmax(scores)\n        return selected_arm\n\n    def update(self, arm, context, reward):\n        self.A[arm] += np.outer(context, context)\n        self.b[arm] += reward * context\n        \n# Assume 'lightweight' is a pandas DataFrame containing your data\n# Assume 'chosen_features' is defined as the list of features for the fighters\nchosen_features = [\n    \"total_comp_time\", \"reach\", \"height\", \"age\",\n    \"knockdowns\", \"sub_attempts\", \"reversals\", \"control\",\n    \"takedowns_landed\", \"takedowns_attempts\",\n    \"sig_strikes_landed\", \"sig_strikes_attempts\",\n    \"total_strikes_landed\", \"total_strikes_attempts\",\n    \"head_strikes_landed\", \"head_strikes_attempts\",\n    \"body_strikes_landed\", \"body_strikes_attempts\",\n    \"leg_strikes_landed\", \"leg_strikes_attempts\",\n    \"distance_strikes_landed\", \"distance_strikes_attempts\",\n    \"clinch_strikes_landed\", \"clinch_strikes_attempts\",\n    \"ground_strikes_landed\", \"ground_strikes_attempts\",\n    \"KO_losses\", \"days_since_last_comp\", \"lose_streak\",\n    \"win_streak\", \"win_loss_ratio\",\n    \"stamina\", \"num_fights\", \"trueskill\", \"elo\"\n]\n\n# Ensure 'result' column exists and indicates the winner (1 for the winner, 0 for the loser)\n\nnum_fights = len(lightweight) // 2\nnum_features = len(chosen_features) * 2  # Features from both fighters\n\n# Initialize variables to track accuracy in chunks\nchunk_size = 100  # Define the size of each chunk for reporting accuracy\nchunk_rewards = [0] * (num_fights // chunk_size + 1)  # List to hold rewards for each chunk\nchunk_correct_predictions = [0] * (num_fights // chunk_size + 1)  # List to hold correct predictions count for each chunk\n\n# Create an instance of the ContextualThompsonSampling class\ncts = ContextualThompsonSampling(2, num_features)\n\ntotal_reward = 0\ncorrect_predictions = 0\n\nfor fight_index in range(num_fights):\n    # Get the records for both fighters in the current fight\n    fighter1_record = lightweight.iloc[fight_index * 2]\n    fighter2_record = lightweight.iloc[fight_index * 2 + 1]\n\n    # Convert features from both fighters to numeric types and handle non-numeric entries\n    fighter1_features = pd.to_numeric(fighter1_record[chosen_features], errors='coerce').fillna(0)\n    fighter2_features = pd.to_numeric(fighter2_record[chosen_features], errors='coerce').fillna(0)\n\n    context = np.concatenate([fighter1_features, fighter2_features])\n    \n    # Use Contextual Thompson Sampling to recommend an arm (fighter)\n    chosen_arm = cts.select_arm(context)\n\n    # Determine the actual winner (arm 0 represents fighter 1 and arm 1 represents fighter 2)\n    actual_winner = 0 if fighter1_record['result'] == 1 else 1\n    \n     # Reward is 1 if the chosen arm matches the actual winner, else 0\n    reward = 1 if chosen_arm == actual_winner else 0\n\n    # Update the model\n    cts.update(chosen_arm, context, reward)\n\n    winning_fighter = \"name\"\n    if actual_winner == 0:\n        winning_fighter = fighter1_record['fighter']\n    else:\n        winning_fighter = fighter2_record['fighter']\n        \n    \n    # Determine the current chunk\n    current_chunk = fight_index // chunk_size\n    \n    # Update rewards and correct predictions for the current chunk\n    chunk_rewards[current_chunk] += reward\n    if chosen_arm == actual_winner:\n        chunk_correct_predictions[current_chunk] += 1\n    \n    # Track total reward and correct predictions\n    total_reward += reward\n    correct_predictions += (chosen_arm == actual_winner)\n\n# Print accuracy for each chunk\nfor i in range(len(chunk_rewards)):\n    if (i + 1) * chunk_size <= num_fights:\n        print(f\"Accuracy for steps {i * chunk_size + 1} - {(i + 1) * chunk_size}: {chunk_correct_predictions[i] / chunk_size}\")\n    else:  # Handle the last chunk which might be smaller than chunk_size\n        print(f\"Accuracy for steps {i * chunk_size + 1} - {num_fights}: {chunk_correct_predictions[i] / (num_fights - i * chunk_size)}\")\n        \n# Calculate accuracy\naccuracy = correct_predictions / num_fights\nprint(f\"Total reward: {total_reward}\")\nprint(f\"Accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:25:52.029310Z","iopub.execute_input":"2023-11-23T06:25:52.030278Z","iopub.status.idle":"2023-11-23T06:26:05.713699Z","shell.execute_reply.started":"2023-11-23T06:25:52.030244Z","shell.execute_reply":"2023-11-23T06:26:05.712268Z"},"trusted":true},"execution_count":null,"outputs":[]}]}